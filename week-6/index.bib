
@article{lin_tsm_2019,
	title = {{TSM}: {Temporal} {Shift} {Module} for {Efficient} {Video} {Understanding}},
	shorttitle = {{TSM}},
	url = {http://arxiv.org/abs/1811.08383},
	abstract = {The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github.com/mit-han-lab/temporal-shift-module.},
	urldate = {2021-01-25},
	journal = {arXiv:1811.08383 [cs]},
	author = {Lin, Ji and Gan, Chuang and Han, Song},
	month = aug,
	year = {2019},
	note = {arXiv: 1811.08383},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\54BCMHI6\\Lin et al. - 2019 - TSM Temporal Shift Module for Efficient Video Und.html:text/html;arXiv Fulltext PDF:D\:\\Documents\\STUDY\\research\\fresh\\storage\\MWN84Z2T\\Lin et al. - 2019 - TSM Temporal Shift Module for Efficient Video Und.pdf:application/pdf},
}

@article{karpathy_large-scale_nodate,
	title = {Large-scale {Video} {Classiﬁcation} with {Convolutional} {Neural} {Networks}},
	abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on largescale video classiﬁcation using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display signiﬁcant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF101 Action Recognition dataset and observe signiﬁcant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
	language = {en},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	pages = {8},
	file = {Karpathy et al. - Large-scale Video Classiﬁcation with Convolutional.pdf:D\:\\Documents\\STUDY\\research\\fresh\\storage\\P9QZXZJZ\\Karpathy et al. - Large-scale Video Classiﬁcation with Convolutional.pdf:application/pdf},
}

@article{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2021-01-27},
	journal = {arXiv:2005.12872 [cs]},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Carion et al_2020_End-to-End Object Detection with Transformers.pdf:D\:\\Documents\\STUDY\\research\\fresh\\storage\\PCFQ9JZT\\Carion et al_2020_End-to-End Object Detection with Transformers.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\R8BYEWT8\\2005.html:text/html;Carion et al_2020_End-to-End Object Detection with Transformers.pdf:D\:\\Documents\\STUDY\\research\\fresh\\storage\\9XCASXK7\\Carion et al_2020_End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2021-01-27},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:D\:\\Documents\\STUDY\\research\\fresh\\storage\\KXQPQSGM\\Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\K8976SBR\\1409.html:text/html},
}
