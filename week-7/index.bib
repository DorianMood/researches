
@article{stanczyk_deep_2021,
	title = {Deep {Graph} {Convolutional} {Networks} for {Wind} {Speed} {Prediction}},
	url = {http://arxiv.org/abs/2101.10041},
	abstract = {Wind speed prediction and forecasting is important for various business and management sectors. In this paper, we introduce new models for wind speed prediction based on graph convolutional networks (GCNs). Given hourly data of several weather variables acquired from multiple weather stations, wind speed values are predicted for multiple time steps ahead. In particular, the weather stations are treated as nodes of a graph whose associated adjacency matrix is learnable. In this way, the network learns the graph spatial structure and determines the strength of relations between the weather stations based on the historical weather data. We add a self-loop connection to the learnt adjacency matrix and normalize the adjacency matrix. We examine two scenarios with the self-loop connection setting (two separate models). In the first scenario, the self-loop connection is imposed as a constant additive. In the second scenario a learnable parameter is included to enable the network to decide about the self-loop connection strength. Furthermore, we incorporate data from multiple time steps with temporal convolution, which together with spatial graph convolution constitutes spatio-temporal graph convolution. We perform experiments on real datasets collected from weather stations located in cities in Denmark and the Netherlands. The numerical experiments show that our proposed models outperform previously developed baseline models on the referenced datasets. We provide additional insights by visualizing learnt adjacency matrices from each layer of our models.},
	urldate = {2021-02-01},
	journal = {arXiv:2101.10041 [cs]},
	author = {Stańczyk, Tomasz and Mehrkanoon, Siamak},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.10041},
	keywords = {Computer Science - Machine Learning, I.2, I.5},
	file = {arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\G47V7GZA\\Stańczyk and Mehrkanoon - 2021 - Deep Graph Convolutional Networks for Wind Speed P.html:text/html;arXiv Fulltext PDF:D\:\\Documents\\STUDY\\research\\fresh\\storage\\CBBXW42G\\Stańczyk and Mehrkanoon - 2021 - Deep Graph Convolutional Networks for Wind Speed P.pdf:application/pdf},
}

@article{zhang_tgcn_2021,
	title = {{TGCN}: {Time} {Domain} {Graph} {Convolutional} {Network} for {Multiple} {Objects} {Tracking}},
	shorttitle = {{TGCN}},
	url = {http://arxiv.org/abs/2101.01861},
	abstract = {Multiple object tracking is to give each object an id in the video. The difficulty is how to match the predicted objects and detected objects in same frames. Matching features include appearance features, location features, etc. These features of the predicted object are basically based on some previous frames. However, few papers describe the relationship in the time domain between the previous frame features and the current frame features.In this paper, we proposed a time domain graph convolutional network for multiple objects tracking.The model is mainly divided into two parts, we first use convolutional neural network (CNN) to extract pedestrian appearance feature, which is a normal operation processing image in deep learning, then we use GCN to model some past frames' appearance feature to get the prediction appearance feature of the current frame. Due to this extension, we can get the pose features of the current frame according to the relationship between some frames in the past. Experimental evaluation shows that our extensions improve the MOTA by 1.3 on the MOT16, achieving overall competitive performance at high frame rates.},
	urldate = {2021-02-01},
	journal = {arXiv:2101.01861 [cs]},
	author = {Zhang, Jie},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.01861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\MBXZZCES\\Zhang - 2021 - TGCN Time Domain Graph Convolutional Network for .html:text/html;arXiv Fulltext PDF:D\:\\Documents\\STUDY\\research\\fresh\\storage\\8V9K3RQC\\Zhang - 2021 - TGCN Time Domain Graph Convolutional Network for .pdf:application/pdf},
}

@inproceedings{araghi_dynamic_2021,
	title = {Dynamic {K}-{Graphs}: an {Algorithm} for {Dynamic} {Graph} {Learning} and {Temporal} {Graph} {Signal} {Clustering}},
	shorttitle = {Dynamic {K}-{Graphs}},
	doi = {10.23919/Eusipco47968.2020.9287661},
	abstract = {Graph signal processing (GSP) have found many applications in different domains. The underlying graph may not be available in all applications, and it should be learned from the data. There exist complicated data, where the graph changes over time. Hence, it is necessary to estimate the dynamic graph. In this paper, a new dynamic graph learning algorithm, called dynamic K -graphs, is proposed. This algorithm is capable of both estimating the time-varying graph and clustering the temporal graph signals. Numerical experiments demonstrate the high performance of this algorithm compared with other algorithms.},
	booktitle = {2020 28th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Araghi, H. and Babaie-Zadeh, M. and Achard, S.},
	month = jan,
	year = {2021},
	note = {ISSN: 2076-1465},
	keywords = {Clustering algorithms, dynamic graph learning, Dynamic K-graphs, dynamic programming, Dynamic programming, graph Laplacian matrix, Heuristic algorithms, Laplace equations, Programming, Signal processing, Signal processing algorithms, temporal graph signal clustering},
	pages = {2195--2199},
	file = {IEEE Xplore Abstract Record:D\:\\Documents\\STUDY\\research\\fresh\\storage\\22W5T2D2\\Araghi et al. - 2021 - Dynamic K-Graphs an Algorithm for Dynamic Graph L.html:text/html;IEEE Xplore Full Text PDF:D\:\\Documents\\STUDY\\research\\fresh\\storage\\BNGQRBW4\\Araghi et al. - 2021 - Dynamic K-Graphs an Algorithm for Dynamic Graph L.pdf:application/pdf},
}

@article{dong_learning_2019,
	title = {Learning graphs from data: {A} signal representation perspective},
	volume = {36},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Learning graphs from data},
	url = {http://arxiv.org/abs/1806.00848},
	doi = {10.1109/MSP.2018.2887284},
	abstract = {The construction of a meaningful graph topology plays a crucial role in the effective representation, processing, analysis and visualization of structured data. When a natural choice of the graph is not readily available from the data sets, it is thus desirable to infer or learn a graph topology from the data. In this tutorial overview, we survey solutions to the problem of graph learning, including classical viewpoints from statistics and physics, and more recent approaches that adopt a graph signal processing (GSP) perspective. We further emphasize the conceptual similarities and differences between classical and GSP-based graph inference methods, and highlight the potential advantage of the latter in a number of theoretical and practical scenarios. We conclude with several open issues and challenges that are keys to the design of future signal processing and machine learning algorithms for learning graphs from data.},
	number = {3},
	urldate = {2021-02-02},
	journal = {IEEE Signal Processing Magazine},
	author = {Dong, Xiaowen and Thanou, Dorina and Rabbat, Michael and Frossard, Pascal},
	month = may,
	year = {2019},
	note = {arXiv: 1806.00848},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	pages = {44--63},
	annote = {Comment: corrected several imprecise statements in previous versions of the manuscript as well as in the article of the same title in the May 2019 issue of IEEE Signal Processing Magazine (vol. 36, no. 3, pp. 44-63, May 2019)},
	file = {Dong et al_2019_Learning graphs from data.pdf:D\:\\Documents\\STUDY\\research\\fresh\\storage\\UNFVRK7Q\\Dong et al_2019_Learning graphs from data.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\5NW3VB9H\\1806.html:text/html},
}

@inproceedings{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	year = {2017},
	note = {ISSN: 10495258
\_eprint: 1706.02216},
	keywords = {basis},
}

@article{wu_simplifying_2019,
	title = {Simplifying {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1902.07153},
	abstract = {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.},
	urldate = {2021-02-02},
	journal = {arXiv:1902.07153 [cs, stat]},
	author = {Wu, Felix and Zhang, Tianyi and Souza Jr., Amauri Holanda de and Fifty, Christopher and Yu, Tao and Weinberger, Kilian Q.},
	month = jun,
	year = {2019},
	note = {arXiv: 1902.07153},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: In ICML 2019. Code available at https://github.com/Tiiiger/SGC},
	file = {Wu et al_2019_Simplifying Graph Convolutional Networks.pdf:D\:\\Documents\\STUDY\\research\\fresh\\storage\\95LRC8QD\\Wu et al_2019_Simplifying Graph Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\VVNFQ56L\\1902.html:text/html},
}


@book{chen_simple_2020,
	title = {Simple and {Deep} {Graph} {Convolutional} {Networks}},
	abstract = {Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the over-smoothing problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: Initial residual and Identity mapping. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at https://github.com/chennnM/GCNII.},
	author = {Chen, Ming and Wei, Zhewei and Huang, Zengfeng and Ding, Bolin and Li, Yaliang},
	year = {2020},
	note = {Publication Title: arXiv
\_eprint: 2007.02133},
}


@article{yang_spagan_2021,
	title = {{SPAGAN}: {Shortest} {Path} {Graph} {Attention} {Network}},
	shorttitle = {{SPAGAN}},
	url = {http://arxiv.org/abs/2101.03464},
	abstract = {Graph convolutional networks (GCN) have recently demonstrated their potential in analyzing non-grid structure data that can be represented as graphs. The core idea is to encode the local topology of a graph, via convolutions, into the feature of a center node. In this paper, we propose a novel GCN model, which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions within each layer, the proposed SPAGAN conducts path-based attention that explicitly accounts for the influence of a sequence of nodes yielding the minimum cost, or shortest path, between the center node and its higher-order neighbors. SPAGAN therefore allows for a more informative and intact exploration of the graph structure and further \{a\} more effective aggregation of information from distant neighbors into the center node, as compared to node-based GCN methods. We test SPAGAN on the downstream classification task on several standard datasets, and achieve performances superior to the state of the art. Code is publicly available at https://github.com/ihollywhy/SPAGAN.},
	urldate = {2021-02-02},
	journal = {arXiv:2101.03464 [cs]},
	author = {Yang, Yiding and Wang, Xinchao and Song, Mingli and Yuan, Junsong and Tao, Dacheng},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.03464},
	keywords = {basis, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted by IJCAI 2019},
	file = {Yang et al_2021_SPAGAN.pdf:D\:\\Documents\\STUDY\\research\\fresh\\storage\\XRDAF4XC\\Yang et al_2021_SPAGAN.pdf:application/pdf;arXiv.org Snapshot:D\:\\Documents\\STUDY\\research\\fresh\\storage\\GQQYQ3FT\\2101.html:text/html},
}

