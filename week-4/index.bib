@misc{Battaglia2018,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences—a hallmark of human intelligence from infancy—remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between “hand-engineering” and “end-to-end” learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias—the graph network—which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have also released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
archivePrefix = {arXiv},
arxivId = {1806.01261},
author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
booktitle = {arXiv},
eprint = {1806.01261},
title = {{Relational inductive biases, deep learning, and graph networks}},
year = {2018}
}
@article{Levie2019,
abstract = {The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach, in comparison to other spectral domain convolutional architectures, on spectral image classification, community detection, vertex classification, and matrix completion tasks.},
archivePrefix = {arXiv},
arxivId = {1705.07664},
author = {Levie, Ron and Monti, Federico and Bresson, Xavier and Bronstein, Michael M.},
doi = {10.1109/TSP.2018.2879624},
eprint = {1705.07664},
file = {:D$\backslash$:/Documents/STUDY/research/week-4/CayleyNets{\_} Graph Convolutional Neural Networks with Complex Rational Spectral Filters.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Geometric deep learning,graph convolution neural networks,graph giltering,spectral approaches},
number = {1},
pages = {97--109},
title = {{CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters}},
volume = {67},
year = {2019}
}
@inproceedings{Malysheva2019,
abstract = {Over recent years, deep reinforcement learning has shown strong successes in complex single-Agent tasks, and more recently this approach has also been applied to multi-Agent domains. In this paper, we propose a novel approach, called MAGNet, to multi-Agent reinforcement learning that utilizes a relevance graph representation of the environment obtained by a self-Attention mechanism, and a message-generation technique. We applied our MAGnet approach to the synthetic predator-prey multi-Agent environment and the Pommerman game and the results show that it significantly outperforms state-of-The-Art MARL solutions, including Multi-Agent Deep Q-Networks (MADQN), Multi-Agent Deep Deterministic Policy Gradient (MADDPG), and QMIX.},
archivePrefix = {arXiv},
arxivId = {2012.09762},
author = {Malysheva, Aleksandra and Kudenko, Daniel and Shpilman, Aleksei},
booktitle = {2019 16th International Symposium "Problems of Redundancy in Information and Control Systems", REDUNDANCY 2019},
doi = {10.1109/REDUNDANCY48165.2019.9003345},
eprint = {2012.09762},
isbn = {9781728119441},
keywords = {deep-learning,multi-Agent system,relevance graphs},
title = {{MAGNet: Multi-Agent Graph Network for Deep Multi-Agent Reinforcement Learning}},
year = {2019}
}
@article{Bai2019,
abstract = {In this paper, we develop a novel Backtrackless Aligned-Spatial Graph Convolutional Network (BASGCN) model to learn effective features for graph classification. Our idea is to transform arbitrary-sized graphs into fixed-sized backtrackless aligned grid structures and define a new spatial graph convolution operation associated with the grid structures. We show that the proposed BASGCN model not only reduces the problems of information loss and imprecise information representation arising in existing spatially-based Graph Convolutional Network (GCN) models, but also bridges the theoretical gap between traditional Convolutional Neural Network (CNN) models and spatially-based GCN models. Furthermore, the proposed BASGCN model can both adaptively discriminate the importance between specified vertices during the convolution process and reduce the notorious tottering problem of existing spatially-based GCNs related to the Weisfeiler-Lehman algorithm, explaining the effectiveness of the proposed model. Experiments on standard graph datasets demonstrate the effectiveness of the proposed model.},
archivePrefix = {arXiv},
arxivId = {1904.04238},
author = {Bai, Lu and Cui, Lixin and Jiao, Yuhang and Rossi, Luca and Hancock, Edwin R.},
doi = {10.1109/tpami.2020.3011866},
eprint = {1904.04238},
file = {:D$\backslash$:/Documents/STUDY/research/week-4/Learning Aligned-Spatial Graph Convolutional Networks for Graph Classification.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Backtrackless Walk,Graph Convolutional Networks,Transitive Vertex Alignment},
pages = {1--16},
title = {{Learning backtrackless aligned-spatial graph convolutional networks for graph classification}},
year = {2019}
}
@misc{AlexanderRush,
abstract = {The Transformer from “Attention is All You Need” has been on a lot of people's minds over the last year. Besides producing major improvements in translation quality, it provides a new architecture for many other NLP tasks. The paper itself is very clearly written, but the conventional wisdom has been that it is quite difficult to implement correctly. In this post I present an “annotated” version of the paper in the form of a line-by-line implementation. I have reordered and deleted some sections from the original paper and added comments throughout. This document itself is a working notebook, and should be a completely usable implementation. In total there are 400 lines of library code which can process 27,000 tokens per second on 4 GPUs. To follow along you will first need to install PyTorch. The complete notebook is also available on github or on Google Colab with free GPUs. Note this is merely a starting point for researchers and interested developers. The code here is based heavily on our OpenNMT packages. (If helpful feel free to cite.) For other full-sevice implementations of the model check-out Tensor2Tensor (tensorflow) and Sockeye (mxnet).},
author = {{Alexander Rush (@harvardnlp or srush@seas.harvard.edu)} and {Vincent Nguyen} and {Guillaume Klein}},
title = {{The Annotated Transformer}},
url = {http://nlp.seas.harvard.edu/2018/04/03/attention.html},
year = {2018}
}
@misc{Chen2020,
abstract = {Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the over-smoothing problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: Initial residual and Identity mapping. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at https://github.com/chennnM/GCNII.},
archivePrefix = {arXiv},
arxivId = {2007.02133},
author = {Chen, Ming and Wei, Zhewei and Huang, Zengfeng and Ding, Bolin and Li, Yaliang},
booktitle = {arXiv},
eprint = {2007.02133},
title = {{Simple and Deep Graph Convolutional Networks}},
year = {2020}
}
@article{Dwivedi2020,
abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
archivePrefix = {arXiv},
arxivId = {2012.09699},
author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
eprint = {2012.09699},
file = {:D$\backslash$:/Documents/STUDY/research/week-4/A Generalization of Transformer Networks to Graphs.pdf:pdf},
title = {{A Generalization of Transformer Networks to Graphs}},
url = {http://arxiv.org/abs/2012.09699},
year = {2020}
}
@inproceedings{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.03762},
issn = {10495258},
title = {{Attention is all you need}},
year = {2017}
}
@inproceedings{Hamilton2017,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.02216},
issn = {10495258},
title = {{Inductive representation learning on large graphs}},
year = {2017}
}
