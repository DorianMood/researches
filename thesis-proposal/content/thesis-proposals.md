---
bibliography: [index.bib]
title: Nikita LS1906205
author:
  - name: Nikita Dolgoshein
    affiliation: Beihang University
csl: [ref.csl]
---

# Introduction

Nowadays there are many existing image compression approaches. We can divide such algorithms in two groups: lossy (such as JPEG. Mainly such algorithms use Discrete Cosine Transform) and lossless (such as DEFLATE. Mainly such algorithms use similar to Huffman Encoding methods). Both has beed developed for decades already. There is a certain trend to apply a power of neural networks in compression domain. First approaches were [@Balle_Laparra_Simoncelli_2017], [@Theis_Shi_Cunningham_Huszar_2017], [@Toderici_Vincent_Johnston_Hwang_Minnen_Shor_Covell_2017], where authors proved that we can use one neural network to generate a compressed representation of an image and another neural network to restore initial image from its representation generated by first network. Now such networks are also being used to solve an image and video upscale tasks.

For now a general approach is to encode an image to compressed representation using convolutional neural network and take the convolutional features from one of the top layers of network. In this work we are going to use a slightly different architecture. There were many publications connected to graph convolution in recent years. Early publications introduced a graph convolution, later the main approaches had been formed. It is important to mention that even though the first application were obviously connected to graph classification and graph clustering [@Kipf_Welling_2017], [@Defferrard_Bresson_Vandergheynst_2017], [@Fey_Lenssen_Weichert_Muller_2018], [@Hamilton_Ying_Leskovec_2017], [@Hamilton_Ying_Leskovec_2018], [@Monti_Boscaini_Masci_Rodolà_Svoboda_Bronstein_2017], [@Monti_Otness_Bronstein_2018], [@Velicković_Cucurull_Casanova_Romero_Liò_Bengio_2017], in recent years application domain has been extended to action recognition [@Cheng_Zhang_He_Chen_Cheng_Lu_2020], supply-demand prediction [@Jin_Xia_Liu_Murata_Kim_2021], time series prediction [@Cheng_Zhang_He_Chen_Cheng_Lu_2020], traffic and wind prediction [@Cui_Henrickson_Ke_Wang_2020], [@Stańczyk_Mehrkanoon_2021] and many others.

In this work we are going to apply graph convolution algorithms to generate a representation of an image that takes less space and can be transported or stored more effectively. The core of this framework is scene graph representation of an image.

# Objective

The first question is how do we memorize some image or visual scene in our life. We do not operate in terms of pixels or object coordinates. More likely we use something like scheme, which describes the most important parts of an image as it shown on [@fig:image_and_scene_graph].

![**Image and scene graph associated with it**](./img/Image%20and%20scene%20graph.png){#fig:image_and_scene_graph}

The scene graph represented on the figure above is simplified, real graph can also have node attributes different from just an object label, for example in case of hair we can also attach an attribute "brown" and "curly".

There are algorithms that can extract a scene graph from an image. There were some approaches to generate image based on scene graph. So the idea is quite obvious: let us combine those two algorithms to build a compression algorithm prototype. From general point of view we can see such algorithm as an autoencoder with Scene Graph Generation network as encoder and Scene Graph to Image network as decoder.

# Approach and methodology

The main contribution of this work is applying a graph convolution in image compression domain. But let us first define what graph convolution actually is. Convolution is already widely used in image and video processing, and mainly convolutional neural networks (CNNs) consist of stacked layers of such convolutions [@Krizhevsky_Sutskever_Hinton_2017], [@Simonyan_Zisserman_2015]. In many architectures authors use learnable *filters* to get representation of the next layer of a network. Convolutional architecture is not new, but it has been widely spread in the last decade because of enormous computation power growth. The main idea of Graph Convolutional Networks is originated from CNNs and generalizes CNN approach from fixed grid structure to arbitrary grid-like graph structure.

In case of this work we can consider scene graph as such structure (@fig:application-general-pipeline) with objects obtained from Object Detection step represented as nodes of this graph, attributes of such nodes can be extracted attributes of an object (here we can simply use output of the last convolutional layer of this object region). Applying graph convolution on such graph we can obtain relationships information, and talking about graphs we can treat such relationships as an edges. So, after this step we obtain so called *Scene Graph*, which represents a meaningful information about image. We can also design a generation model and then restore image from such representation. The image obtained from the last step will not be identical, and the purpose of this work is to design such an algorithm that will produce similar enough image.

![**Application general pipeline.** First scene graph and convolution features are obtained. Then this can be treated as a compressed representation of initial image. It can be sent through network or being stored on machine. When the original image is needed we can feed this representation to generative model to obtain initial image.](./img/Application%20general%20pipeline.png){#fig:application-general-pipeline}

Application general pipeline can be divided on three steps:

1. Scene Graph generation. Having an image as an input on this step we need to generate a scene graph of this image. Here is an example of an image and its scene graph (@fig:image-and-scene-graph)
2. Image generation based on scene graph. Having a scene graph on this step we need to generate (or restore) image.

![**Image and its scene graph**](./img/Image%20and%20scene%20graph.png){#fig:image-and-scene-graph}

The first thing we want to do is object detection. We need labeled objects regions, that were detected from image. Now there are many various object algorithms, they are already quite advanced and we can simply choose one of them and use as it is. There are a couple of possible choices: FastRCNN, Object detection with transformers, YOLOv5 etc.

After object detection algorithm applied we will have a list of regions and the classes for each region (@fig:objects-detected), we will also need a convolutional features for each region (it will be just a small vector of values obtained from top convolutional layer). Having all the listed we have enough information to build a scene graph and restore image when it is needed. mMore concrete we can use information about detected object regions as a hint to restore original image.

![**Example of detected objects using FastRCNN**](./img/Object%20detected.png){#fig:objects-detected}

Next we start building a scene graph. Scene graph can be built using different approaches, but we decided to use an approach with graph convolutional networks, since it shows a nice performance and, probably, graph convolution has a bit of potential now. There are still many publications in this sphere.

As we can see from (@fig:scene-graph-example), first objects are detected and then they are filtered and scene graph is obtained.

![**Scene graph generation general view.** The left part is original image, the middle part is image with detected objects and the right part is scene graph that can be obtained from such image.](./img/Scene%20graph%20example.png){#fig:scene-graph-example}

Scene graph generation is not an easy task, so let us introduce a basic scene graph generation pipeline [@fig:sgg-pipeline]. It's a pity that it is not possible to use such algorithm out-of-the-box, so we will need to implement one of scene graph generation algorithms.

As an input such algorithms usually take an image with detected objects. It is not important what algorithm has been used to detect objects and regions. objects are filtered to get rid of small and redundant objects. So once we have a filtered objects it means that we have three different parts of this detection:

1. Object labels and attributes.
2. Object regions, usually in rectangle coordinate form.
3. Convolutional features from top convolutional layer.

Next step is to connect each object with all the others, so we have a fully-connected bipartite graph. Those connections are graph edges and they are also called relationships proposals. After that step relationships proposals are being filtered. Basically it means that algorithm compares two related objects and if they are likely not related to each other, then it will remove this relation from relations list. Then after obtaining filtered relationships we can build an unlabeled version of scene graph (with no relation labels). To label relations we apply a Graph Convolutional network, which will find a mapping of corresponding labels for each relation. It's worth to mention that the final graph is directed. If there is a relation form first object to second it doesn't mean that there is a relation from second to first.

![**General pipeline to generate a scene graph based on objects detected from original image.**](./img/SGG%20pipeline.png){#fig:sgg-pipeline}

Having a scene graph and detected objects (objects labels and attributes, objects regions, objects convolutional features) and some convolutional feature of a scene, we can then use this information to restore an image. Figure below [@fig:image-generation-from-sg] shows a general pipeline for such task.

![**General pipeline to generate an image from an arbitrary scene graph.**](./img/Image%20generation%20from%20SG.png){#fig:image-generation-from-sg}

As we can see from the figure, we first take a scene graph and based on objects information we build a scheme of scene, we basically map a scene to know were are the objects located and what features they have. Then such map is combined with scene convolutional features and fed in generative model, that will generate an image.

# Data

We decided to use Visual Genome dataset, which has been used in many other works [@Krishna_Zhu_Groth_Johnson_Hata_Kravitz_Chen_Kalantidis_Li_Shamma_etal_2016]. Visual genome consists of more than 100,000 images labeled with scene graphs.

Dataset consists of several main parts:

1. Images.
2. Detected objects information (coordinates of rectangles for each object, objects labels, objects attributes).
3. Scene graphs, obtained from images.

Images are in JPEG format with various height and width.

# Potential outcomes

Potentially this work can improve existing image compression algorithms, since the size of scene graph is way more small than size of JPEG (proof???). Such an algorithm can be used in different scenarios. We selected two possible ones:

1. Limited space on machine with high computational power. It can be a big server with powerful CPU. In recent days there are some movements in quantum computing sphere, so it can be a very powerful machine with quantum CPU etc.
2. Huge images needed to be transmitted through a network with limited speed. In case of a limited performance of a network we can compress and restore images using application, we are working on.

# Potential limitations

The main limitation is needed to mention. Since we still don't have even the simplest version of an algorithm, we still cannot see the quality of restored images, there can be a possible problem. This problem can be solved by storing an additional information about an image such as scene and objects convolutional features. In this work we are going to ballance this information size and quality of restored images.

# Contributions

The contribution of this work can be considered from two major perspective. The first is that this work is going to be the first try to use scene graph as a compressed representation of an image. From the second perspective we are going to use a general autoencoder architecture with meaningful graph structure as a representation. 

# Dissertation chapters

# Reference