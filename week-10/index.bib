 @article{Chang_Ren_Xu_Li_Chen_Hauptmann_2021,
  title        = {Scene Graphs: A Survey of Generations and Applications},
  url          = {http://arxiv.org/abs/2104.01111},
  abstractnote = {Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisﬁed with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also know the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and ﬁnd similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More speciﬁcally, we ﬁrst summarized the general deﬁnition of the scene graph, then conducted a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigated the main applications of scene graphs and summarized the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs. We believe this will be a very helpful foundation for future research on scene graphs.},
  note         = {arXiv: 2104.01111},
  journal      = {arXiv:2104.01111 [cs]},
  author       = {Chang, Xiaojun and Ren, Pengzhen and Xu, Pengfei and Li, Zhihui and Chen, Xiaojiang and Hauptmann, Alex},
  year         = {2021},
  month        = {Mar}
}
 @inbook{Zhang_Wang_Guo_2019,
  place        = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {Scene Graph Generation via Convolutional Message Passing and Class-Aware Memory Embeddings},
  volume       = {11729},
  isbn         = {978-3-030-30507-9},
  url          = {http://link.springer.com/10.1007/978-3-030-30508-6_49},
  doi          = {10.1007/978-3-030-30508-6_49},
  abstractnote = {Detecting visual relationships between objects in an image still remains challenging, because the relationships are diﬃcult to be modeled and the class imbalance problem tends to jeopardize the predictions. To alleviate these problems, we propose an end-to-end approach for scene graph generations. The proposed method employs the ResNet as the backbone network to extract the appearance features of the objects and relationships. An attention based graph convolutional network is exploited and modiﬁed to extract the contextual information. Language and geometric priors are also utilized and fused with the visual features to better describe the relationships. At last, a novel memory module is designed to alleviate the class imbalance problem. Experimental results demonstrate the validity of our model and our superiority compared to our baseline technique.},
  booktitle    = {Artificial Neural Networks and Machine Learning – ICANN 2019: Image Processing},
  publisher    = {Springer International Publishing},
  author       = {Zhang, Yidong and Wang, Yunhong and Guo, Yuanfang},
  editor       = {Tetko, Igor V. and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},
  year         = {2019},
  pages        = {620–633},
  collection   = {Lecture Notes in Computer Science}
}
 @article{Ren_He_Girshick_Sun_2016,
  title        = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  url          = {http://arxiv.org/abs/1506.01497},
  abstractnote = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  note         = {arXiv: 1506.01497},
  journal      = {arXiv:1506.01497 [cs]},
  author       = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year         = {2016},
  month        = {Jan}
}
 @article{Li_Ouyang_Zhou_Shi_Zhang_Wang_2018,
  title        = {Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation},
  url          = {http://arxiv.org/abs/1806.11538},
  abstractnote = {Generating scene graph to describe all the relations inside an image gains increasing interests these years. However, most of the previous methods use complicated structures with slow inference speed or rely on the external data, which limits the usage of the model in real-life scenarios. To improve the efficiency of scene graph generation, we propose a subgraph-based connection graph to concisely represent the scene graph during the inference. A bottom-up clustering method is first used to factorize the entire scene graph into subgraphs, where each subgraph contains several objects and a subset of their relationships. By replacing the numerous relationship representations of the scene graph with fewer subgraph and object features, the computation in the intermediate stage is significantly reduced. In addition, spatial information is maintained by the subgraph features, which is leveraged by our proposed Spatial-weighted Message Passing~(SMP) structure and Spatial-sensitive Relation Inference~(SRI) module to facilitate the relationship recognition. On the recent Visual Relationship Detection and Visual Genome datasets, our method outperforms the state-of-the-art method in both accuracy and speed.},
  note         = {arXiv: 1806.11538},
  journal      = {arXiv:1806.11538 [cs]},
  author       = {Li, Yikang and Ouyang, Wanli and Zhou, Bolei and Shi, Jianping and Zhang, Chao and Wang, Xiaogang},
  year         = {2018},
  month        = {Aug}
}
 @article{Yang_Lu_Lee_Batra_Parikh_2018,
  title        = {Graph R-CNN for Scene Graph Generation},
  url          = {http://arxiv.org/abs/1808.00191},
  abstractnote = {We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.},
  note         = {arXiv: 1808.00191},
  journal      = {arXiv:1808.00191 [cs]},
  author       = {Yang, Jianwei and Lu, Jiasen and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
  year         = {2018},
  month        = {Aug}
}

